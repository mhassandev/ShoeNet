{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include the packages before training ShoeNet\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')#GTK', 'GTKAgg', 'GTKCairo', 'GTK3Agg', 'GTK3Cairo', 'MacOSX', 'nbAgg', 'Qt4Agg', 'Qt4Cairo', \n",
    "#'Qt5Agg', 'Qt5Cairo', 'TkAgg', 'TkCairo', 'WebAgg', 'WX', 'WXAgg', 'WXCairo', 'agg', 'cairo', 'gdk', 'pdf', 'pgf', \n",
    "#'ps', 'svg', 'template'\n",
    "import matplotlib.pyplot as plt\n",
    "# For example, here's several helpful packages to load in \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from pandas import read_excel\n",
    "import xlrd\n",
    "import math\n",
    "import concurrent.futures\n",
    "from skimage.io import imread\n",
    "import os.path\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, upsample_2d,avg_pool_2d,grouped_conv_2d , conv_2d_transpose, batch_normalization, atrous_conv_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "from tflearn.layers.normalization import local_response_normalization#\n",
    "#from tflearn.optimizers import sgd, rmsprop, adadelta, adagrad, Adam\n",
    "from os import listdir\n",
    "input_data = tflearn.input_data\n",
    "fc = tflearn.fully_connected\n",
    "bn = tflearn.batch_normalization\n",
    "activation = tflearn.activation\n",
    "merge = tflearn.merge\n",
    "regression = tflearn.regression\n",
    "DNN = tflearn.DNN\n",
    "from sys import exit\n",
    "import sys\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread,imshow\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.morphology import label\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "input_data = tflearn.input_data\n",
    "fc = tflearn.fully_connected\n",
    "bn = tflearn.batch_normalization\n",
    "activation = tflearn.activation\n",
    "merge = tflearn.merge\n",
    "regression = tflearn.regression\n",
    "DNN = tflearn.DNN\n",
    "from sys import exit\n",
    "import sys\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "from tflearn.layers.normalization import local_response_normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage as sk\n",
    "import scipy.ndimage as nd\n",
    "import skimage as sk\n",
    "import scipy.ndimage as nd\n",
    "#Fucntions which have been used in augmentation\n",
    "def img_rotate(imgr):\n",
    "    imgr=nd.rotate(imgr,8)\n",
    "    imgresize=cv2.resize(imgr, (224,224))\n",
    "    return(imgresize)\n",
    "def img_crop(img):\n",
    "    new_img=cv2.resize(img,(224,224,), interpolation = cv2.INTER_LANCZOS4)\n",
    "    new_img=new_img[8:216,8:216]\n",
    "    new_img=cv2.resize(img,(224,224,), interpolation = cv2.INTER_CUBIC)\n",
    "    return(new_img)\n",
    "def img_blur(img):\n",
    "    nimg=blurImg = cv2.blur(img,(5,5))\n",
    "    return(nimg)\n",
    "#////////////////////////////////flip image//////////////////////////////////\n",
    "def img_flip_ltr(imgf):\n",
    "    imgf=cv2.flip(imgf,1)\n",
    "    imgf=cv2.resize(imgf, (224,224), interpolation = cv2.INTER_CUBIC)\n",
    "    return(imgf)\n",
    "def img_noise(imgn):\n",
    "    return(cv2.resize(sk.util.random_noise(imgn, mode='gaussian', seed=None, clip=True), (224,224),interpolation = cv2.INTER_LANCZOS4))\n",
    "def img_flip_up(imgf):\n",
    "    imgf=cv2.flip(imgf,0)\n",
    "    imgf=np.array(imgf)\n",
    "    imgf=cv2.resize(imgf, (224,224))\n",
    "    return(imgf)\n",
    "def norm_image_return(imgn):\n",
    "    imgn=cv2.resize(imgn,(224,224))\n",
    "    return(imgn)\n",
    "import skimage\n",
    "def img_noise0(imgn):\n",
    "    return(cv2.resize(sk.util.random_noise(imgn, mode='s&p', seed=None, clip=True), (224,224), interpolation = cv2.INTER_CUBIC))\n",
    "#guassain,localvar, poisson, salt, pepper, s&p,speckle\n",
    "def img_noise1(imgn):\n",
    "    return(cv2.resize(sk.util.random_noise(imgn, mode='localvar', seed=None,var=1, clip=True), (224,224),interpolation = cv2.INTER_CUBIC))\n",
    "import skimage\n",
    "def cimage(img):\n",
    "    cimg=skimage.color.rgb2gray(img)\n",
    "    cimg=cv2.resize(cimg, (112,224), interpolation = cv2.INTER_CUBIC)\n",
    "    return(cimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading training and testing dataset\n",
    "training=np.load('traing_file_path.npy')\n",
    "testdata=np.load('tessting_file_path.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totl_data=#generating balance dataset/augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating input images(trX) and their corresponding labels(trY)\n",
    "trX=[]\n",
    "trY=[]\n",
    "for i in totl_data:\n",
    "    trX.append(i[0])\n",
    "    trY.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-arranging for the training samples for training\n",
    "trX=np.reshape(trX,(-1,224,224,1))\n",
    "trY=np.reshape(trY,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the relted packages for training the model in tensorfow/tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import tflearn\n",
    "import tflearn.activations as activations\n",
    "# Data loading and preprocessing\n",
    "import tflearn.datasets.oxflower17 as oxflower17\n",
    "from tflearn.activations import relu\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import dropout, flatten, fully_connected, input_data\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "from tflearn.utils import repeat\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.losses as tf_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For automatice augmentation during run time, when have used pre-augmentation\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "img_aug = ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "#img_aug1.add_random_flip_updown()\n",
    "img_aug.add_random_rotation(max_angle=8.)\n",
    "#img_aug.add_random_90degrees_rotation(rotations=[0,1,2,3])\n",
    "img_aug.add_random_blur(sigma_max=4.0)\n",
    "img_aug.add_random_crop((224,224), 4)\n",
    "img_prep=tflearn.ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center()# img_prep.add_featurewise_zero_cneter(mean=[0.4,0.4,0.4])\n",
    "img_prep.add_featurewise_stdnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model (ShoeNet) architecture\n",
    "network_in = input_data(shape=[None, 224, 224, 1], data_preprocessing=None, data_augmentation=None, name='Input1')\n",
    "network_in = tflearn.batch_normalization(network_in)\n",
    "network_in = tflearn.activations.relu(network_in)\n",
    "network3 = conv_2d(network_in, 32, 3, activation='relu')\n",
    "network3 = conv_2d(network3, 32, 3, activation='relu')\n",
    "network3a = conv_2d(network3, 32, 3, activation='relu')\n",
    "\n",
    "network1 = tflearn.batch_normalization(network3a)\n",
    "network1a = tflearn.activations.relu(network1)\n",
    "networktf = max_pool_2d(network1a, 2)#downsample by 2---112*112\n",
    "\n",
    "network64 = conv_2d(networktf, 64, 3, activation='relu')\n",
    "network64 = conv_2d(network64, 64, 3, activation='relu')\n",
    "network64 = conv_2d(network64, 64, 3, activation='relu')\n",
    "network64f = max_pool_2d(network64, 2)#downsample by 2---56*56\n",
    "\n",
    "network64a = conv_2d(network3a, 32, 1, activation='relu')\n",
    "network64a = max_pool_2d(network64a, 4)\n",
    "network64c = merge([network64f,network64a], mode='concat', axis=3)\n",
    "network64c = tflearn.batch_normalization(network64c)\n",
    "network64d = tflearn.activations.relu(network64c)\n",
    "\n",
    "network128 = conv_2d(network64d, 128, 3, activation='relu')\n",
    "network128 = conv_2d(network128, 128, 3, activation='relu')\n",
    "network128 = conv_2d(network128, 128, 3, activation='relu')\n",
    "network128f = max_pool_2d(network128, 2)#downsample by 2-----28*28\n",
    "\n",
    "network128a = conv_2d(network3a, 32, 1, activation='relu')\n",
    "network128a = max_pool_2d(network128a, 8)\n",
    "network128b = conv_2d(network64d, 32, 1, activation='relu')\n",
    "network128b = max_pool_2d(network128b, 2)#downsample by 2\n",
    "network128c= merge([network128f, network128a,network128b], mode='concat', axis=3)\n",
    "\n",
    "network128c = tflearn.batch_normalization(network128c)\n",
    "network128c = tflearn.activations.relu(network128c)\n",
    "\n",
    "network256 = conv_2d(network128c, 256, 3, activation='relu')\n",
    "network256 = conv_2d(network256, 256, 3, activation='relu')\n",
    "network256 = conv_2d(network256, 256, 3, activation='relu')\n",
    "network256f = max_pool_2d(network256, 2)#downsample by 2-----14*14\n",
    "\n",
    "network256a = conv_2d(network3a, 32, 1, activation='relu')\n",
    "network256a = max_pool_2d(network256a, 16)\n",
    "network256d = conv_2d(network64d, 32, 1, activation='relu')\n",
    "network256d = max_pool_2d(network256d, 4)\n",
    "network256e = conv_2d(network128c, 32, 1, activation='relu')\n",
    "network256e = max_pool_2d(network256e, 2)#downsample by 2\n",
    "network256c= merge([network256f, network256a,network256d,network256e], mode='concat', axis=3)\n",
    "\n",
    "network256c = tflearn.batch_normalization(network256c)\n",
    "network256c = tflearn.activations.relu(network256c)\n",
    "\n",
    "network512 = conv_2d(network256c, 512, 3, activation='relu')\n",
    "network512 = conv_2d(network512, 512, 3, activation='relu')\n",
    "network512 = conv_2d(network512, 512, 3, activation='relu')\n",
    "network512f = max_pool_2d(network512, 2)#downsample by 2---7*7\n",
    "\n",
    "network512a = conv_2d(network3a, 32, 1, activation='relu')\n",
    "network512a = max_pool_2d(network512a, 32)\n",
    "network512d = conv_2d(network64d, 32, 1, activation='relu')\n",
    "network512d = max_pool_2d(network512d, 8)\n",
    "network512e = conv_2d(network128c, 32, 1, activation='relu')\n",
    "network512e = max_pool_2d(network512e, 4)\n",
    "network512g = conv_2d(network256c, 32, 1, activation='relu')\n",
    "network512g = max_pool_2d(network512g, 2)#downsample by 2\n",
    "network512cc= merge([network512f, network512a,network512d,network512e,network512g], mode='concat', axis=3)\n",
    "\n",
    "\n",
    "network512c = tflearn.batch_normalization(network512cc)\n",
    "network512c = tflearn.activations.relu(network512c)\n",
    "network512c = dropout(network512c, 0.8)\n",
    "\n",
    "network512c = fully_connected(network512c, 384, activation='relu')\n",
    "network512c = dropout(network512c, 0.7)\n",
    "network = fully_connected(network512c, 384, activation='relu')\n",
    "network = dropout(network, 0.6)\n",
    "network = fully_connected(network, 384, activation='relu', regularizer='L2', weight_decay=0.0001)\n",
    "network = dropout(network, 0.5)\n",
    "networko = fully_connected(network, 1, activation='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "adam=tflearn.optimizers.Adam(learning_rate=0.001, beta1=0.99, beta2=0.999, epsilon=1e-08, use_locking=False, name='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss fucntion\n",
    "def clf_m(labels, predictions, k=2.0):\n",
    "    diff=tf.reduce_mean(tf.abs(predictions-labels))\n",
    "    lossout = tf.reduce_mean(tf.where(diff<=k, (0.000001*diff), (diff**3)+0.1))\n",
    "    return (lossout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feeding the optimizer(Adam), loss fuction(clf_m), meric for regression(R2), learning rate(0.0001)\n",
    "convnet = regression(networko, optimizer=adam,loss=clf_m, metric='R2' ,name='targets', learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep neural network with including tensorboard verbose(0, for best visualization use 3), path for the saving tensorboard\n",
    "modelb = tflearn.DNN(convnet, tensorboard_verbose=0,tensorboard_dir='/tensorboard_directoryrd_logfiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model with Input(trX=shoeprints), target(trY=corresponding ages), no_of_epoch, valiation set(0.1, 10% of the\n",
    "#of the training dataset),  snapshotp will be taken after 1000 steps, enable the metric to show the values, batch_size(84, \n",
    "#it can ben increase or decrease based on the available GPU memory, Enable shuffling for the training samples)\n",
    "no_of_epoch=give the number of epoch\n",
    "modelb.fit({'Input':trX}, {'target':trY}, n_epoch=no_of_epoch, validation_set=0.1, snapshot_step=1000, show_metric=True,\n",
    "           batch_size=84,shuffle=True, run_id='model_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
